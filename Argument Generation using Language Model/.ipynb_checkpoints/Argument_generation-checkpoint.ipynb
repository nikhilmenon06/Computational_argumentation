{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9BdcMJxkW3HJ",
    "outputId": "93ce81b6-a77a-4a08-e181-2dc14a49b615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KTRd_NhhOy6L"
   },
   "outputs": [],
   "source": [
    "# Paths to data files and output file on drive\n",
    "train_data_file = '/content/drive/MyDrive/train_data.json'\n",
    "val_data_file = '/content/drive/MyDrive/valid_data.json' #needs to be modified for test file\n",
    "pred_out_file = '/content/drive/MyDrive/prediction_out.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dz3sRO0_X9If"
   },
   "outputs": [],
   "source": [
    "# Import all dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import json\n",
    "from transformers import PegasusTokenizer, TFPegasusForConditionalGeneration\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "\n",
    "model = TFPegasusForConditionalGeneration.from_pretrained('google/pegasus-xsum')\n",
    "tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-xsum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "ZLLJULWBYA55",
    "outputId": "8f69c523-4874-4c0e-a590-7a8f1a471033"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conclusion</th>\n",
       "      <th>id</th>\n",
       "      <th>argument</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Slang prevents young people to become potentia...</td>\n",
       "      <td>51640</td>\n",
       "      <td>People want to hire potential employers that a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tesla Motors should have a dealer network, eve...</td>\n",
       "      <td>61343</td>\n",
       "      <td>Tesla Motors have mostly been in the business ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Earthlings could be \"early bloomers\", the firs...</td>\n",
       "      <td>93408</td>\n",
       "      <td>Radiation in the cosmos has decreased That has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I don't think income equality is a problem.</td>\n",
       "      <td>108747</td>\n",
       "      <td>I don't think income inequality is a problem. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A UBI wouldn't have to be paid for by taxes. A...</td>\n",
       "      <td>47729</td>\n",
       "      <td>An economy using a fiat currency is not revenu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          conclusion  ...                                           argument\n",
       "0  Slang prevents young people to become potentia...  ...  People want to hire potential employers that a...\n",
       "1  Tesla Motors should have a dealer network, eve...  ...  Tesla Motors have mostly been in the business ...\n",
       "2  Earthlings could be \"early bloomers\", the firs...  ...  Radiation in the cosmos has decreased That has...\n",
       "3        I don't think income equality is a problem.  ...  I don't think income inequality is a problem. ...\n",
       "4  A UBI wouldn't have to be paid for by taxes. A...  ...  An economy using a fiat currency is not revenu...\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train_init = pd.read_json(train_data_file)\n",
    "df_val_init = pd.read_json(val_data_file)\n",
    "\n",
    "display(df_val_init.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OB0lCvcgpPfP"
   },
   "outputs": [],
   "source": [
    "# Contraction mapping dictionary obtained from internet\n",
    "\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                          \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MFp7cbxUdsW4"
   },
   "outputs": [],
   "source": [
    "# Function to Preprocess the text data: Lower Casing, Remove URLs, punctuations and stripping extra spaces\n",
    "def PreprocessData(df):\n",
    "  for column in ['conclusion','argument']:\n",
    "      df['clean_' + column] = df[column].str.lower()\n",
    "      df['clean_' + column] = df['clean_' + column].str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ') #remove URL\n",
    "      df['clean_' + column] = df['clean_' + column].apply(lambda x: ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in x.split()]))\n",
    "      df['clean_' + column] = df['clean_' + column].str.strip()\n",
    "      df['clean_' + column] = df['clean_' + column].str.replace('[^\\w\\s]','')\n",
    "      df['clean_' + column] = df['clean_' + column].str.replace('\\n','')\n",
    "      # df['clean_' + column] = df['clean_' + column].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords]))\n",
    "  return df\n",
    "\n",
    "# df['response'] = df['response'].apply(lambda x: [item for item in x.split() if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "l816hMk5Zh7x"
   },
   "outputs": [],
   "source": [
    "df_train_clean = PreprocessData(df_train_init)\n",
    "df_val_clean = PreprocessData(df_val_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "rGNtTu-yZqaC",
    "outputId": "13e25c03-30ab-4dd2-882b-256c8a28948f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conclusion</th>\n",
       "      <th>id</th>\n",
       "      <th>argument</th>\n",
       "      <th>clean_conclusion</th>\n",
       "      <th>clean_argument</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Slang prevents young people to become potentia...</td>\n",
       "      <td>51640</td>\n",
       "      <td>People want to hire potential employers that a...</td>\n",
       "      <td>slang prevents young people to become potentia...</td>\n",
       "      <td>people want to hire potential employers that a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tesla Motors should have a dealer network, eve...</td>\n",
       "      <td>61343</td>\n",
       "      <td>Tesla Motors have mostly been in the business ...</td>\n",
       "      <td>tesla motors should have a dealer network even...</td>\n",
       "      <td>tesla motors have mostly been in the business ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Earthlings could be \"early bloomers\", the firs...</td>\n",
       "      <td>93408</td>\n",
       "      <td>Radiation in the cosmos has decreased That has...</td>\n",
       "      <td>earthlings could be early bloomers the first t...</td>\n",
       "      <td>radiation in the cosmos has decreased that has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I don't think income equality is a problem.</td>\n",
       "      <td>108747</td>\n",
       "      <td>I don't think income inequality is a problem. ...</td>\n",
       "      <td>i do not think income equality is a problem</td>\n",
       "      <td>i do not think income inequality is a problem ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A UBI wouldn't have to be paid for by taxes. A...</td>\n",
       "      <td>47729</td>\n",
       "      <td>An economy using a fiat currency is not revenu...</td>\n",
       "      <td>a ubi would not have to be paid for by taxes a...</td>\n",
       "      <td>an economy using a fiat currency is not revenu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          conclusion  ...                                     clean_argument\n",
       "0  Slang prevents young people to become potentia...  ...  people want to hire potential employers that a...\n",
       "1  Tesla Motors should have a dealer network, eve...  ...  tesla motors have mostly been in the business ...\n",
       "2  Earthlings could be \"early bloomers\", the firs...  ...  radiation in the cosmos has decreased that has...\n",
       "3        I don't think income equality is a problem.  ...  i do not think income inequality is a problem ...\n",
       "4  A UBI wouldn't have to be paid for by taxes. A...  ...  an economy using a fiat currency is not revenu...\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_val_clean.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "DgO6dOkk83-W"
   },
   "outputs": [],
   "source": [
    "# # Argument tokenizer\n",
    "\n",
    "# # Fit the tokenzier on train data\n",
    "# arg_tokenizer = Tokenizer(oov_token=1)\n",
    "# arg_tokenizer.fit_on_texts(df_train_clean['clean_argument'].values)\n",
    "\n",
    "# # Transform the texts to integer sequences - train and val data\n",
    "# x_train_arg = arg_tokenizer.texts_to_sequences(df_train_clean['clean_argument'].values)\n",
    "# x_val_arg = arg_tokenizer.texts_to_sequences(df_val_clean['clean_argument'].values)\n",
    "\n",
    "# # Pad length for uniformity\n",
    "# max_length = max(len(s.split()) for s in df_train_clean['clean_argument'].values)\n",
    "# x_train_arg = pad_sequences(x_train_arg,maxlen=max_length)\n",
    "# x_val_arg = pad_sequences(x_val_arg,maxlen=max_length)\n",
    "\n",
    "# # Conclusion tokenizer\n",
    "\n",
    "# # Fit the tokenzier on train data\n",
    "# conc_tokenizer = Tokenizer(oov_token=1)\n",
    "# conc_tokenizer.fit_on_texts(df_train_clean['clean_conclusion'].values)\n",
    "\n",
    "# # Transform the texts to integer sequences - train and val data\n",
    "# x_train_conc = conc_tokenizer.texts_to_sequences(df_train_clean['clean_conclusion'].values)\n",
    "# x_val_conc = conc_tokenizer.texts_to_sequences(df_val_clean['clean_conclusion'].values)\n",
    "\n",
    "# # Pad length for uniformity\n",
    "# max_length = max(len(s.split()) for s in df_train_clean['clean_conclusion'].values)\n",
    "# x_train_conc = pad_sequences(x_train_conc,maxlen=max_length)\n",
    "# x_val_conc = pad_sequences(x_val_conc,maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qjtJLASm1k9M"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Argument_generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
